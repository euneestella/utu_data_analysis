{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 차원 축소(dimensionality rdeuction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "University of Turku의 Data Analysis and Knowledge Discovery 강의 내용을 개인적으로 정리한 글입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 왜 차원 축소가 필요할까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터를 분석하다 보면 매우 많은 변수들이 있는, 그러니까 고차원의 데이터를 쉽게 볼 수 있다. 데이터가 담고 있는 정보량이 많다는 이야기인데, 많을수록 좋은 게 아닐까? 회귀분석을 했을 때 혼동변수를 여러 개 더할수록 예쁜 관계가 그려지고 R-squared 값이 커져서 좋아했던 기억이 난다. <u>그런데 왜 차원 축소가 필요할까?</u>\n",
    "\n",
    "그런데 머신러닝에서는 지나치게 많은 차원을 지양하며, 이를 <b>차원의 저주</b>(curse of dimension)이라고 부르기도 한다. 주어진 데이터의 양은 정해져 있지만 차원이 커지게 되면 데이터가 존재할 수 있는 공간이 넓어지고 데이터의 밀도가 줄어들어 모델의 타당성이 떨어지게 된다.\n",
    "\n",
    "차원의 저주 외에도 <b>시각화</b>의 어려움을 들 수 있겠다. 고차원의 데이터는 시각화하기 매우 어렵다. 그런데 우리가 친숙한 (예컨대) 2차원 공간으로 그 데이터를 특징의 왜곡 없이 데려온다면 시각화가 매우 편리할 것이다.\n",
    "\n",
    "이 외에도 차원이 줄어들면 <b>연산의 속도</b>가 빨라질 테니 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 차원 축소의 기본 아이디어\n",
    "차원 축소의 기본적인 아이디어는 n차원 공간의 데이터를 더 낮은 q차원 공간의 데이터로 변환(transform)하는 사상(mapping)을 찾는 것이다. 단, 이때 사상은 <u>데이터의 원래 구조를 보존</u>하는 방식이어야 한다. \n",
    "\n",
    "\n",
    "사상(寫像)의 한자어를 풀어 보면 寫(베낄 사)와 像(모양 상)으로, 구조(모양)을 보존(베끼다)한다는 위의 의미를 더 잘 이해할 수 있을 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 주성분 분석(Principal Component Analysis, PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "차원 축소를 위한 대표적인 방법이다. 주성분 분석에 대해 소개하기 전에 고유벡터를 짚고 가자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 고유벡터(Eigenvector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주성분 분석의 핵심 아이디어는 선형대수 시간에 배웠던 <u>고유벡터</u>이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Mona_Lisa_eigenvector_grid.png/480px-Mona_Lisa_eigenvector_grid.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "왼쪽에 주어진 그림을 오른쪽처럼 늘려서 선형사상(linear mapping)한다고 해 보자. 이때 빨간색 벡터는 방향이 변하지만, 파란색 벡터는 선형사상에도 불구하고 방향이 변하지 않는데, 이런 파란색 벡터와 같은 것을 <u>고유벡터</u>라고 한다. 나는 이걸 선형 변환을 해도 유지되는 <u>특징적인 벡터</u>라고 이해했다. 참고로 [wikipedia](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors)에서도 eigenvector를 characteristic vector라고 소개한다.\n",
    "\n",
    "조금 더 수학적으로 말해보자면"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Ax\\quad =\\quad \\lambda x $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>(𝐴는 nxn 행렬, 𝜆는 스칼라, 𝑥는 벡터)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "를 만족하는 영벡터가 아닌 벡터 𝑥가 존재하면 스칼라 𝜆를 행렬 𝐴의 <u>고윳값</u>, 벡터 𝑥를 고윳값 𝜆에 대응하는 <u>고유벡터</u>라고 한다. \n",
    "\n",
    "그러니 고윳값을 찾는 행렬 𝐴의 역할이 중요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](img/pca_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그림으로 이해해 보면 PC1 벡터와 PC2 벡터는 3차원에서 2차원으로 차원이 줄어들었음에도 불구하고 벡터의 특징이 그대로 살아있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주성분 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주성분 분석의 목표를 다시 정리해 보자.   \n",
    "<b>'데이터의 차원을 줄여도, 구조가 보존되는 고유벡터를 찾자!'</b>    \n",
    "    \n",
    "우리가 찾고 싶어하는 고유벡터가 바로 주성분(principal component, PC)이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터의 <u>구조를 보존</u>하는 것이 미션이다. 그러니까 개별 데이터 하나하나보다는 데이터의 <b>분포</b>, 데이터와 데이터의 <b>관계</b>를 보아야 한다. 그러면 우리에게 남은 선택지는 상관(correlation)과 공분산(covariance)이다.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 상관(correlation) vs 공분산(covariance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "둘 다 사용할 수 있는 것으로 보인다. 두 방법을 사용했을 때 주성분 분석의 결과가 어떻게 다른지는 다음의 [링크](https://towardsdatascience.com/let-us-understand-the-correlation-matrix-and-covariance-matrix-d42e6b643c22)를 참고하면 좋다. 다만 변수의 단위가 비슷할 때에는 공분산을, 다를 때에는 상관을 사용하는 것이 좋다.     \n",
    "\n",
    "다만 일반적인 주성분분석에서는 분석에 앞서 <u>정규화를 한 다음 <b>공분산 행렬</b>을 사용</u>한다. 상관은 편리하지만 두 변수 사이 관계가 비선형이라면 관계를 제대로 파악할 수 없기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 주성분 분석의 단계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주성분 분석의 핵심이 <u>고유벡터</u>이고, 고유벡터를 구하기 위해서는\n",
    "$$ Ax\\quad =\\quad \\lambda x $$\n",
    "에서 행렬 𝐴의 역할이 중요하다고 했다. 주성분 분석에서의 행렬 𝐴는 <u>공분산 행렬</u>이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 데이터의 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Z\\quad =\\quad \\begin{bmatrix} { x }_{ 1* }\\quad -\\quad \\mu  \\\\ \\vdots  \\\\ { x }_{ m* }\\quad -\\quad \\mu  \\end{bmatrix} $$\n",
    "\n",
    "앞서 말했듯 공분산 행렬은 변수의 단위 문제에 취약하다. 이를 극복하기 위해 평균을 0으로 만드는 정규화를 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "m = X.shape[0]\n",
    "Z = X - np.mean(X, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 공분산 행렬의 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ C\\quad =\\quad \\frac { 1 }{ m-1 } { Z }^{ T }Z $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 데이터들에 대한 공분산 행렬을 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = Z.T@Z/(m-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이썬에서 transpose는 ```A.T```, 행렬의 곱은 ```A@B```으로 나타낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 공분산 행렬의 고윳값 분해"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ C\\quad =\\quad \\left( { v }_{ 1 },\\quad \\cdots \\quad ,{ v }_{ m } \\right) \\begin{pmatrix} { \\lambda  }_{ 1 } &  &  \\\\  & \\ddots  &  \\\\  &  & { \\lambda  }_{ m } \\end{pmatrix}\\begin{pmatrix} { v }_{ 1 }^{ T } \\\\ \\vdots  \\\\ { v }_{ m }^{ T } \\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "공분산 행렬 𝐶의 고유벡터를 찾기 위해 고윳값 분해를 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals, evecs = np.linalg.eigh(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 첫 q개의 고유벡터 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "고유벡터를 고윳값이 큰 순으로 나열한다. 고유벡터는 데이터의 차원 수만큼 생긴다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argsort(evals)[::-1]\n",
    "evecs = evecs[:,idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "축소하고자 하는 차원의 수만큼 고유벡터를 선택한다. 예컨대 2차원으로 축소하고 싶으면 2개의 고유벡터를 고른다. 첫번째 고유벡터(주성분)을 PC1, 두번째 고유벡터를 PC2라고 해보자.\n",
    "\n",
    "고윳값 분해를 하는 행렬이 공분산 행렬이므로, PC1는 가장 큰 분산을 나타내며, PC2는 PC1와 서로 수직(orthogonal)이다. 이를 통해 데이터의 차원을 축소시키면서 동시에 특징을 가장 잘 드러내는(공분산이 큰) 특징들을 남겼다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = evecs[:,:2].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 데이터 사영"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PC1와 PC2 벡터는 서로 수직이므로 두 벡터를 새로운 축으로 삼을 수 있겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://wiki.math.uwaterloo.ca/statwiki/images/4/4f/PCA_in_Neuroscience.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 구한 고유벡터와 원 데이터를 내적해서 최종적으로 차원을 축소한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ M{ Z }^{ T } $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_mapped = (M@Z.T).T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
